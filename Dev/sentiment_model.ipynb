{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\gurde\\Anaconda3\\envs\\deloitte\n",
      "\n",
      "  added / updated specs:\n",
      "    - gensim\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    boto3-1.9.66               |           py37_0         106 KB\n",
      "    botocore-1.12.189          |             py_0         3.4 MB\n",
      "    bz2file-0.98               |           py37_1          14 KB\n",
      "    ca-certificates-2020.1.1   |                0         125 KB\n",
      "    certifi-2019.11.28         |           py37_0         154 KB\n",
      "    docutils-0.16              |           py37_0         667 KB\n",
      "    gensim-3.8.0               |   py37hf9181ef_0        18.4 MB\n",
      "    jmespath-0.9.4             |             py_0          22 KB\n",
      "    s3transfer-0.1.13          |           py37_0          79 KB\n",
      "    smart_open-1.9.0           |             py_0          59 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        23.0 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  boto               pkgs/main/win-64::boto-2.49.0-py37_0\n",
      "  boto3              pkgs/main/win-64::boto3-1.9.66-py37_0\n",
      "  botocore           pkgs/main/noarch::botocore-1.12.189-py_0\n",
      "  bz2file            pkgs/main/win-64::bz2file-0.98-py37_1\n",
      "  docutils           pkgs/main/win-64::docutils-0.16-py37_0\n",
      "  gensim             pkgs/main/win-64::gensim-3.8.0-py37hf9181ef_0\n",
      "  jmespath           pkgs/main/noarch::jmespath-0.9.4-py_0\n",
      "  s3transfer         pkgs/main/win-64::s3transfer-0.1.13-py37_0\n",
      "  smart_open         pkgs/main/noarch::smart_open-1.9.0-py_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  ca-certificates                                  anaconda --> pkgs/main\n",
      "  certifi                                          anaconda --> pkgs/main\n",
      "  openssl                anaconda::openssl-1.1.1-he774522_0 --> pkgs/main::openssl-1.1.1d-he774522_4\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "ca-certificates-2020 | 125 KB    |            |   0% \n",
      "ca-certificates-2020 | 125 KB    | #2         |  13% \n",
      "ca-certificates-2020 | 125 KB    | ##5        |  26% \n",
      "ca-certificates-2020 | 125 KB    | ###8       |  38% \n",
      "ca-certificates-2020 | 125 KB    | #####1     |  51% \n",
      "ca-certificates-2020 | 125 KB    | ######3    |  64% \n",
      "ca-certificates-2020 | 125 KB    | #######6   |  77% \n",
      "ca-certificates-2020 | 125 KB    | ########9  |  89% \n",
      "ca-certificates-2020 | 125 KB    | ########## | 100% \n",
      "\n",
      "smart_open-1.9.0     | 59 KB     |            |   0% \n",
      "smart_open-1.9.0     | 59 KB     | ##7        |  27% \n",
      "smart_open-1.9.0     | 59 KB     | #####4     |  55% \n",
      "smart_open-1.9.0     | 59 KB     | ########1  |  82% \n",
      "smart_open-1.9.0     | 59 KB     | ########## | 100% \n",
      "\n",
      "docutils-0.16        | 667 KB    |            |   0% \n",
      "docutils-0.16        | 667 KB    | 2          |   2% \n",
      "docutils-0.16        | 667 KB    | 4          |   5% \n",
      "docutils-0.16        | 667 KB    | 9          |  10% \n",
      "docutils-0.16        | 667 KB    | #4         |  14% \n",
      "docutils-0.16        | 667 KB    | #9         |  19% \n",
      "docutils-0.16        | 667 KB    | ##6        |  26% \n",
      "docutils-0.16        | 667 KB    | ###1       |  31% \n",
      "docutils-0.16        | 667 KB    | ###5       |  36% \n",
      "docutils-0.16        | 667 KB    | ####       |  41% \n",
      "docutils-0.16        | 667 KB    | ####5      |  46% \n",
      "docutils-0.16        | 667 KB    | #####      |  50% \n",
      "docutils-0.16        | 667 KB    | #####5     |  55% \n",
      "docutils-0.16        | 667 KB    | ######2    |  62% \n",
      "docutils-0.16        | 667 KB    | ######7    |  67% \n",
      "docutils-0.16        | 667 KB    | #######4   |  74% \n",
      "docutils-0.16        | 667 KB    | #######9   |  79% \n",
      "docutils-0.16        | 667 KB    | ########3  |  84% \n",
      "docutils-0.16        | 667 KB    | ########8  |  89% \n",
      "docutils-0.16        | 667 KB    | #########3 |  94% \n",
      "docutils-0.16        | 667 KB    | #########8 |  98% \n",
      "docutils-0.16        | 667 KB    | ########## | 100% \n",
      "\n",
      "bz2file-0.98         | 14 KB     |            |   0% \n",
      "bz2file-0.98         | 14 KB     | ########## | 100% \n",
      "\n",
      "certifi-2019.11.28   | 154 KB    |            |   0% \n",
      "certifi-2019.11.28   | 154 KB    | #          |  10% \n",
      "certifi-2019.11.28   | 154 KB    | ##         |  21% \n",
      "certifi-2019.11.28   | 154 KB    | ###1       |  31% \n",
      "certifi-2019.11.28   | 154 KB    | #####2     |  52% \n",
      "certifi-2019.11.28   | 154 KB    | ######2    |  63% \n",
      "certifi-2019.11.28   | 154 KB    | #######2   |  73% \n",
      "certifi-2019.11.28   | 154 KB    | #########3 |  94% \n",
      "certifi-2019.11.28   | 154 KB    | ########## | 100% \n",
      "\n",
      "botocore-1.12.189    | 3.4 MB    |            |   0% \n",
      "botocore-1.12.189    | 3.4 MB    |            |   0% \n",
      "botocore-1.12.189    | 3.4 MB    |            |   1% \n",
      "botocore-1.12.189    | 3.4 MB    | 1          |   2% \n",
      "botocore-1.12.189    | 3.4 MB    | 2          |   3% \n",
      "botocore-1.12.189    | 3.4 MB    | 4          |   4% \n",
      "botocore-1.12.189    | 3.4 MB    | 5          |   5% \n",
      "botocore-1.12.189    | 3.4 MB    | 5          |   6% \n",
      "botocore-1.12.189    | 3.4 MB    | 6          |   7% \n",
      "botocore-1.12.189    | 3.4 MB    | 7          |   8% \n",
      "botocore-1.12.189    | 3.4 MB    | 8          |   9% \n",
      "botocore-1.12.189    | 3.4 MB    | 9          |  10% \n",
      "botocore-1.12.189    | 3.4 MB    | #          |  11% \n",
      "botocore-1.12.189    | 3.4 MB    | #1         |  12% \n",
      "botocore-1.12.189    | 3.4 MB    | #2         |  13% \n",
      "botocore-1.12.189    | 3.4 MB    | #3         |  14% \n",
      "botocore-1.12.189    | 3.4 MB    | #4         |  15% \n",
      "botocore-1.12.189    | 3.4 MB    | #5         |  16% \n",
      "botocore-1.12.189    | 3.4 MB    | #6         |  17% \n",
      "botocore-1.12.189    | 3.4 MB    | #7         |  17% \n",
      "botocore-1.12.189    | 3.4 MB    | #8         |  18% \n",
      "botocore-1.12.189    | 3.4 MB    | #9         |  19% \n",
      "botocore-1.12.189    | 3.4 MB    | ##         |  20% \n",
      "botocore-1.12.189    | 3.4 MB    | ##1        |  21% \n",
      "botocore-1.12.189    | 3.4 MB    | ##2        |  23% \n",
      "botocore-1.12.189    | 3.4 MB    | ##3        |  23% \n",
      "botocore-1.12.189    | 3.4 MB    | ##5        |  25% \n",
      "botocore-1.12.189    | 3.4 MB    | ##6        |  26% \n",
      "botocore-1.12.189    | 3.4 MB    | ##7        |  28% \n",
      "botocore-1.12.189    | 3.4 MB    | ##8        |  29% \n",
      "botocore-1.12.189    | 3.4 MB    | ##9        |  29% \n",
      "botocore-1.12.189    | 3.4 MB    | ###        |  30% \n",
      "botocore-1.12.189    | 3.4 MB    | ###1       |  31% \n",
      "botocore-1.12.189    | 3.4 MB    | ###2       |  32% \n",
      "botocore-1.12.189    | 3.4 MB    | ###3       |  33% \n",
      "botocore-1.12.189    | 3.4 MB    | ###4       |  34% \n",
      "botocore-1.12.189    | 3.4 MB    | ###4       |  35% \n",
      "botocore-1.12.189    | 3.4 MB    | ###5       |  36% \n",
      "botocore-1.12.189    | 3.4 MB    | ###6       |  37% \n",
      "botocore-1.12.189    | 3.4 MB    | ###7       |  38% \n",
      "botocore-1.12.189    | 3.4 MB    | ###8       |  39% \n",
      "botocore-1.12.189    | 3.4 MB    | ###9       |  40% \n",
      "botocore-1.12.189    | 3.4 MB    | ####       |  41% \n",
      "botocore-1.12.189    | 3.4 MB    | ####1      |  41% \n",
      "botocore-1.12.189    | 3.4 MB    | ####2      |  42% \n",
      "botocore-1.12.189    | 3.4 MB    | ####3      |  44% \n",
      "botocore-1.12.189    | 3.4 MB    | ####4      |  45% \n",
      "botocore-1.12.189    | 3.4 MB    | ####5      |  46% \n",
      "botocore-1.12.189    | 3.4 MB    | ####6      |  47% \n",
      "botocore-1.12.189    | 3.4 MB    | ####7      |  47% \n",
      "botocore-1.12.189    | 3.4 MB    | ####8      |  48% \n",
      "botocore-1.12.189    | 3.4 MB    | ####9      |  49% \n",
      "botocore-1.12.189    | 3.4 MB    | ####9      |  50% \n",
      "botocore-1.12.189    | 3.4 MB    | #####      |  51% \n",
      "botocore-1.12.189    | 3.4 MB    | #####1     |  51% \n",
      "botocore-1.12.189    | 3.4 MB    | #####3     |  53% \n",
      "botocore-1.12.189    | 3.4 MB    | #####4     |  54% \n",
      "botocore-1.12.189    | 3.4 MB    | #####6     |  57% \n",
      "botocore-1.12.189    | 3.4 MB    | #####7     |  58% \n",
      "botocore-1.12.189    | 3.4 MB    | #####8     |  58% \n",
      "botocore-1.12.189    | 3.4 MB    | #####9     |  59% \n",
      "botocore-1.12.189    | 3.4 MB    | ######     |  60% \n",
      "botocore-1.12.189    | 3.4 MB    | ######1    |  61% \n",
      "botocore-1.12.189    | 3.4 MB    | ######2    |  62% \n",
      "botocore-1.12.189    | 3.4 MB    | ######3    |  63% \n",
      "botocore-1.12.189    | 3.4 MB    | ######5    |  65% \n",
      "botocore-1.12.189    | 3.4 MB    | ######6    |  67% \n",
      "botocore-1.12.189    | 3.4 MB    | ######9    |  69% \n",
      "botocore-1.12.189    | 3.4 MB    | ######9    |  70% \n",
      "botocore-1.12.189    | 3.4 MB    | #######    |  71% \n",
      "botocore-1.12.189    | 3.4 MB    | #######1   |  72% \n",
      "botocore-1.12.189    | 3.4 MB    | #######4   |  74% \n",
      "botocore-1.12.189    | 3.4 MB    | #######5   |  76% \n",
      "botocore-1.12.189    | 3.4 MB    | #######6   |  76% \n",
      "botocore-1.12.189    | 3.4 MB    | #######7   |  78% \n",
      "botocore-1.12.189    | 3.4 MB    | #######8   |  79% \n",
      "botocore-1.12.189    | 3.4 MB    | ########   |  80% \n",
      "botocore-1.12.189    | 3.4 MB    | ########1  |  81% \n",
      "botocore-1.12.189    | 3.4 MB    | ########2  |  82% \n",
      "botocore-1.12.189    | 3.4 MB    | ########3  |  83% \n",
      "botocore-1.12.189    | 3.4 MB    | ########5  |  85% \n",
      "botocore-1.12.189    | 3.4 MB    | ########6  |  87% \n",
      "botocore-1.12.189    | 3.4 MB    | ########7  |  88% \n",
      "botocore-1.12.189    | 3.4 MB    | ########9  |  89% \n",
      "botocore-1.12.189    | 3.4 MB    | #########  |  91% \n",
      "botocore-1.12.189    | 3.4 MB    | #########2 |  92% \n",
      "botocore-1.12.189    | 3.4 MB    | #########3 |  93% \n",
      "botocore-1.12.189    | 3.4 MB    | #########4 |  95% \n",
      "botocore-1.12.189    | 3.4 MB    | #########7 |  98% \n",
      "botocore-1.12.189    | 3.4 MB    | #########9 |  99% \n",
      "botocore-1.12.189    | 3.4 MB    | ########## | 100% \n",
      "\n",
      "boto3-1.9.66         | 106 KB    |            |   0% \n",
      "boto3-1.9.66         | 106 KB    | #5         |  15% \n",
      "boto3-1.9.66         | 106 KB    | ######     |  61% \n",
      "boto3-1.9.66         | 106 KB    | #######5   |  76% \n",
      "boto3-1.9.66         | 106 KB    | ########## | 100% \n",
      "\n",
      "s3transfer-0.1.13    | 79 KB     |            |   0% \n",
      "s3transfer-0.1.13    | 79 KB     | ##         |  20% \n",
      "s3transfer-0.1.13    | 79 KB     | ######     |  61% \n",
      "s3transfer-0.1.13    | 79 KB     | ########## | 100% \n",
      "\n",
      "jmespath-0.9.4       | 22 KB     |            |   0% \n",
      "jmespath-0.9.4       | 22 KB     | #######3   |  74% \n",
      "jmespath-0.9.4       | 22 KB     | ########## | 100% \n",
      "\n",
      "gensim-3.8.0         | 18.4 MB   |            |   0% \n",
      "gensim-3.8.0         | 18.4 MB   |            |   0% \n",
      "gensim-3.8.0         | 18.4 MB   |            |   0% \n",
      "gensim-3.8.0         | 18.4 MB   |            |   0% \n",
      "gensim-3.8.0         | 18.4 MB   |            |   1% \n",
      "gensim-3.8.0         | 18.4 MB   |            |   1% \n",
      "gensim-3.8.0         | 18.4 MB   |            |   1% \n",
      "gensim-3.8.0         | 18.4 MB   |            |   1% \n",
      "gensim-3.8.0         | 18.4 MB   |            |   1% \n",
      "gensim-3.8.0         | 18.4 MB   | 1          |   1% \n",
      "gensim-3.8.0         | 18.4 MB   | 1          |   1% \n",
      "gensim-3.8.0         | 18.4 MB   | 1          |   1% \n",
      "gensim-3.8.0         | 18.4 MB   | 1          |   1% \n",
      "gensim-3.8.0         | 18.4 MB   | 1          |   2% \n",
      "gensim-3.8.0         | 18.4 MB   | 1          |   2% \n",
      "gensim-3.8.0         | 18.4 MB   | 1          |   2% \n",
      "gensim-3.8.0         | 18.4 MB   | 2          |   2% \n",
      "gensim-3.8.0         | 18.4 MB   | 2          |   3% \n",
      "gensim-3.8.0         | 18.4 MB   | 2          |   3% \n",
      "gensim-3.8.0         | 18.4 MB   | 2          |   3% \n",
      "gensim-3.8.0         | 18.4 MB   | 3          |   3% \n",
      "gensim-3.8.0         | 18.4 MB   | 3          |   3% \n",
      "gensim-3.8.0         | 18.4 MB   | 3          |   4% \n",
      "gensim-3.8.0         | 18.4 MB   | 3          |   4% \n",
      "gensim-3.8.0         | 18.4 MB   | 4          |   4% \n",
      "gensim-3.8.0         | 18.4 MB   | 4          |   4% \n",
      "gensim-3.8.0         | 18.4 MB   | 4          |   5% \n",
      "gensim-3.8.0         | 18.4 MB   | 4          |   5% \n",
      "gensim-3.8.0         | 18.4 MB   | 4          |   5% \n",
      "gensim-3.8.0         | 18.4 MB   | 5          |   5% \n",
      "gensim-3.8.0         | 18.4 MB   | 5          |   5% \n",
      "gensim-3.8.0         | 18.4 MB   | 5          |   6% \n",
      "gensim-3.8.0         | 18.4 MB   | 5          |   6% \n",
      "gensim-3.8.0         | 18.4 MB   | 6          |   6% \n",
      "gensim-3.8.0         | 18.4 MB   | 6          |   6% \n",
      "gensim-3.8.0         | 18.4 MB   | 6          |   7% \n",
      "gensim-3.8.0         | 18.4 MB   | 6          |   7% \n",
      "gensim-3.8.0         | 18.4 MB   | 7          |   7% \n",
      "gensim-3.8.0         | 18.4 MB   | 7          |   7% \n",
      "gensim-3.8.0         | 18.4 MB   | 7          |   8% \n",
      "gensim-3.8.0         | 18.4 MB   | 7          |   8% \n",
      "gensim-3.8.0         | 18.4 MB   | 8          |   8% \n",
      "gensim-3.8.0         | 18.4 MB   | 8          |   8% \n",
      "gensim-3.8.0         | 18.4 MB   | 8          |   9% \n",
      "gensim-3.8.0         | 18.4 MB   | 8          |   9% \n",
      "gensim-3.8.0         | 18.4 MB   | 9          |   9% \n",
      "gensim-3.8.0         | 18.4 MB   | 9          |   9% \n",
      "gensim-3.8.0         | 18.4 MB   | 9          |  10% \n",
      "gensim-3.8.0         | 18.4 MB   | 9          |  10% \n",
      "gensim-3.8.0         | 18.4 MB   | #          |  10% \n",
      "gensim-3.8.0         | 18.4 MB   | #          |  10% \n",
      "gensim-3.8.0         | 18.4 MB   | #          |  11% \n",
      "gensim-3.8.0         | 18.4 MB   | #1         |  11% \n",
      "gensim-3.8.0         | 18.4 MB   | #1         |  11% \n",
      "gensim-3.8.0         | 18.4 MB   | #1         |  12% \n",
      "gensim-3.8.0         | 18.4 MB   | #1         |  12% \n",
      "gensim-3.8.0         | 18.4 MB   | #2         |  12% \n",
      "gensim-3.8.0         | 18.4 MB   | #2         |  12% \n",
      "gensim-3.8.0         | 18.4 MB   | #2         |  13% \n",
      "gensim-3.8.0         | 18.4 MB   | #3         |  13% \n",
      "gensim-3.8.0         | 18.4 MB   | #3         |  13% \n",
      "gensim-3.8.0         | 18.4 MB   | #3         |  14% \n",
      "gensim-3.8.0         | 18.4 MB   | #3         |  14% \n",
      "gensim-3.8.0         | 18.4 MB   | #4         |  14% \n",
      "gensim-3.8.0         | 18.4 MB   | #4         |  14% \n",
      "gensim-3.8.0         | 18.4 MB   | #4         |  14% \n",
      "gensim-3.8.0         | 18.4 MB   | #4         |  15% \n",
      "gensim-3.8.0         | 18.4 MB   | #4         |  15% \n",
      "gensim-3.8.0         | 18.4 MB   | #5         |  16% \n",
      "gensim-3.8.0         | 18.4 MB   | #6         |  16% \n",
      "gensim-3.8.0         | 18.4 MB   | #6         |  16% \n",
      "gensim-3.8.0         | 18.4 MB   | #6         |  17% \n",
      "gensim-3.8.0         | 18.4 MB   | #7         |  17% \n",
      "gensim-3.8.0         | 18.4 MB   | #7         |  18% \n",
      "gensim-3.8.0         | 18.4 MB   | #7         |  18% \n",
      "gensim-3.8.0         | 18.4 MB   | #8         |  18% \n",
      "gensim-3.8.0         | 18.4 MB   | #8         |  19% \n",
      "gensim-3.8.0         | 18.4 MB   | #9         |  19% \n",
      "gensim-3.8.0         | 18.4 MB   | #9         |  20% \n",
      "gensim-3.8.0         | 18.4 MB   | ##         |  20% \n",
      "gensim-3.8.0         | 18.4 MB   | ##         |  21% \n",
      "gensim-3.8.0         | 18.4 MB   | ##1        |  21% \n",
      "gensim-3.8.0         | 18.4 MB   | ##1        |  21% \n",
      "gensim-3.8.0         | 18.4 MB   | ##1        |  22% \n",
      "gensim-3.8.0         | 18.4 MB   | ##1        |  22% \n",
      "gensim-3.8.0         | 18.4 MB   | ##2        |  22% \n",
      "gensim-3.8.0         | 18.4 MB   | ##2        |  22% \n",
      "gensim-3.8.0         | 18.4 MB   | ##2        |  23% \n",
      "gensim-3.8.0         | 18.4 MB   | ##2        |  23% \n",
      "gensim-3.8.0         | 18.4 MB   | ##3        |  23% \n",
      "gensim-3.8.0         | 18.4 MB   | ##3        |  23% \n",
      "gensim-3.8.0         | 18.4 MB   | ##3        |  24% \n",
      "gensim-3.8.0         | 18.4 MB   | ##3        |  24% \n",
      "gensim-3.8.0         | 18.4 MB   | ##4        |  24% \n",
      "gensim-3.8.0         | 18.4 MB   | ##4        |  25% \n",
      "gensim-3.8.0         | 18.4 MB   | ##4        |  25% \n",
      "gensim-3.8.0         | 18.4 MB   | ##5        |  25% \n",
      "gensim-3.8.0         | 18.4 MB   | ##5        |  25% \n",
      "gensim-3.8.0         | 18.4 MB   | ##5        |  26% \n",
      "gensim-3.8.0         | 18.4 MB   | ##6        |  26% \n",
      "gensim-3.8.0         | 18.4 MB   | ##6        |  27% \n",
      "gensim-3.8.0         | 18.4 MB   | ##7        |  27% \n",
      "gensim-3.8.0         | 18.4 MB   | ##7        |  27% \n",
      "gensim-3.8.0         | 18.4 MB   | ##7        |  28% \n",
      "gensim-3.8.0         | 18.4 MB   | ##8        |  28% \n",
      "gensim-3.8.0         | 18.4 MB   | ##8        |  29% \n",
      "gensim-3.8.0         | 18.4 MB   | ##8        |  29% \n",
      "gensim-3.8.0         | 18.4 MB   | ##8        |  29% \n",
      "gensim-3.8.0         | 18.4 MB   | ##9        |  29% \n",
      "gensim-3.8.0         | 18.4 MB   | ##9        |  29% \n",
      "gensim-3.8.0         | 18.4 MB   | ##9        |  29% \n",
      "gensim-3.8.0         | 18.4 MB   | ##9        |  30% \n",
      "gensim-3.8.0         | 18.4 MB   | ##9        |  30% \n",
      "gensim-3.8.0         | 18.4 MB   | ###        |  30% \n",
      "gensim-3.8.0         | 18.4 MB   | ###        |  30% \n",
      "gensim-3.8.0         | 18.4 MB   | ###        |  30% \n",
      "gensim-3.8.0         | 18.4 MB   | ###        |  31% \n",
      "gensim-3.8.0         | 18.4 MB   | ###        |  31% \n",
      "gensim-3.8.0         | 18.4 MB   | ###        |  31% \n",
      "gensim-3.8.0         | 18.4 MB   | ###1       |  31% \n",
      "gensim-3.8.0         | 18.4 MB   | ###1       |  31% \n",
      "gensim-3.8.0         | 18.4 MB   | ###1       |  31% \n",
      "gensim-3.8.0         | 18.4 MB   | ###1       |  32% \n",
      "gensim-3.8.0         | 18.4 MB   | ###1       |  32% \n",
      "gensim-3.8.0         | 18.4 MB   | ###1       |  32% \n",
      "gensim-3.8.0         | 18.4 MB   | ###2       |  32% \n",
      "gensim-3.8.0         | 18.4 MB   | ###2       |  32% \n",
      "gensim-3.8.0         | 18.4 MB   | ###2       |  33% \n",
      "gensim-3.8.0         | 18.4 MB   | ###2       |  33% \n",
      "gensim-3.8.0         | 18.4 MB   | ###2       |  33% \n",
      "gensim-3.8.0         | 18.4 MB   | ###3       |  33% \n",
      "gensim-3.8.0         | 18.4 MB   | ###3       |  33% \n",
      "gensim-3.8.0         | 18.4 MB   | ###3       |  33% \n",
      "gensim-3.8.0         | 18.4 MB   | ###3       |  34% \n",
      "gensim-3.8.0         | 18.4 MB   | ###3       |  34% \n",
      "gensim-3.8.0         | 18.4 MB   | ###3       |  34% \n",
      "gensim-3.8.0         | 18.4 MB   | ###4       |  34% \n",
      "gensim-3.8.0         | 18.4 MB   | ###4       |  34% \n",
      "gensim-3.8.0         | 18.4 MB   | ###4       |  35% \n",
      "gensim-3.8.0         | 18.4 MB   | ###4       |  35% \n",
      "gensim-3.8.0         | 18.4 MB   | ###5       |  35% \n",
      "gensim-3.8.0         | 18.4 MB   | ###5       |  35% \n",
      "gensim-3.8.0         | 18.4 MB   | ###5       |  36% \n",
      "gensim-3.8.0         | 18.4 MB   | ###5       |  36% \n",
      "gensim-3.8.0         | 18.4 MB   | ###6       |  36% \n",
      "gensim-3.8.0         | 18.4 MB   | ###6       |  37% \n",
      "gensim-3.8.0         | 18.4 MB   | ###7       |  37% \n",
      "gensim-3.8.0         | 18.4 MB   | ###7       |  37% \n",
      "gensim-3.8.0         | 18.4 MB   | ###7       |  38% \n",
      "gensim-3.8.0         | 18.4 MB   | ###8       |  38% \n",
      "gensim-3.8.0         | 18.4 MB   | ###8       |  39% \n",
      "gensim-3.8.0         | 18.4 MB   | ###9       |  39% \n",
      "gensim-3.8.0         | 18.4 MB   | ###9       |  39% \n",
      "gensim-3.8.0         | 18.4 MB   | ###9       |  40% \n",
      "gensim-3.8.0         | 18.4 MB   | ####       |  40% \n",
      "gensim-3.8.0         | 18.4 MB   | ####       |  41% \n",
      "gensim-3.8.0         | 18.4 MB   | ####       |  41% \n",
      "gensim-3.8.0         | 18.4 MB   | ####1      |  41% \n",
      "gensim-3.8.0         | 18.4 MB   | ####1      |  42% \n",
      "gensim-3.8.0         | 18.4 MB   | ####2      |  42% \n",
      "gensim-3.8.0         | 18.4 MB   | ####2      |  43% \n",
      "gensim-3.8.0         | 18.4 MB   | ####2      |  43% \n",
      "gensim-3.8.0         | 18.4 MB   | ####3      |  43% \n",
      "gensim-3.8.0         | 18.4 MB   | ####3      |  44% \n",
      "gensim-3.8.0         | 18.4 MB   | ####4      |  44% \n",
      "gensim-3.8.0         | 18.4 MB   | ####4      |  44% \n",
      "gensim-3.8.0         | 18.4 MB   | ####4      |  45% \n",
      "gensim-3.8.0         | 18.4 MB   | ####5      |  45% \n",
      "gensim-3.8.0         | 18.4 MB   | ####5      |  46% \n",
      "gensim-3.8.0         | 18.4 MB   | ####6      |  46% \n",
      "gensim-3.8.0         | 18.4 MB   | ####6      |  46% \n",
      "gensim-3.8.0         | 18.4 MB   | ####6      |  47% \n",
      "gensim-3.8.0         | 18.4 MB   | ####7      |  47% \n",
      "gensim-3.8.0         | 18.4 MB   | ####7      |  48% \n",
      "gensim-3.8.0         | 18.4 MB   | ####8      |  48% \n",
      "gensim-3.8.0         | 18.4 MB   | ####8      |  49% \n",
      "gensim-3.8.0         | 18.4 MB   | ####9      |  49% \n",
      "gensim-3.8.0         | 18.4 MB   | ####9      |  49% \n",
      "gensim-3.8.0         | 18.4 MB   | ####9      |  50% \n",
      "gensim-3.8.0         | 18.4 MB   | ####9      |  50% \n",
      "gensim-3.8.0         | 18.4 MB   | #####      |  50% \n",
      "gensim-3.8.0         | 18.4 MB   | #####1     |  51% \n",
      "gensim-3.8.0         | 18.4 MB   | #####1     |  52% \n",
      "gensim-3.8.0         | 18.4 MB   | #####1     |  52% \n",
      "gensim-3.8.0         | 18.4 MB   | #####2     |  52% \n",
      "gensim-3.8.0         | 18.4 MB   | #####2     |  53% \n",
      "gensim-3.8.0         | 18.4 MB   | #####3     |  53% \n",
      "gensim-3.8.0         | 18.4 MB   | #####4     |  54% \n",
      "gensim-3.8.0         | 18.4 MB   | #####4     |  55% \n",
      "gensim-3.8.0         | 18.4 MB   | #####5     |  55% \n",
      "gensim-3.8.0         | 18.4 MB   | #####5     |  56% \n",
      "gensim-3.8.0         | 18.4 MB   | #####6     |  56% \n",
      "gensim-3.8.0         | 18.4 MB   | #####6     |  57% \n",
      "gensim-3.8.0         | 18.4 MB   | #####7     |  57% \n",
      "gensim-3.8.0         | 18.4 MB   | #####8     |  58% \n",
      "gensim-3.8.0         | 18.4 MB   | #####8     |  59% \n",
      "gensim-3.8.0         | 18.4 MB   | #####8     |  59% \n",
      "gensim-3.8.0         | 18.4 MB   | #####9     |  59% \n",
      "gensim-3.8.0         | 18.4 MB   | #####9     |  59% \n",
      "gensim-3.8.0         | 18.4 MB   | #####9     |  60% \n",
      "gensim-3.8.0         | 18.4 MB   | ######     |  60% \n",
      "gensim-3.8.0         | 18.4 MB   | ######     |  60% \n",
      "gensim-3.8.0         | 18.4 MB   | ######     |  61% \n",
      "gensim-3.8.0         | 18.4 MB   | ######1    |  61% \n",
      "gensim-3.8.0         | 18.4 MB   | ######1    |  62% \n",
      "gensim-3.8.0         | 18.4 MB   | ######2    |  62% \n",
      "gensim-3.8.0         | 18.4 MB   | ######2    |  63% \n",
      "gensim-3.8.0         | 18.4 MB   | ######3    |  63% \n",
      "gensim-3.8.0         | 18.4 MB   | ######3    |  64% \n",
      "gensim-3.8.0         | 18.4 MB   | ######4    |  64% \n",
      "gensim-3.8.0         | 18.4 MB   | ######5    |  66% \n",
      "gensim-3.8.0         | 18.4 MB   | ######6    |  66% \n",
      "gensim-3.8.0         | 18.4 MB   | ######6    |  67% \n",
      "gensim-3.8.0         | 18.4 MB   | ######7    |  68% \n",
      "gensim-3.8.0         | 18.4 MB   | ######8    |  68% \n",
      "gensim-3.8.0         | 18.4 MB   | ######8    |  69% \n",
      "gensim-3.8.0         | 18.4 MB   | ######9    |  69% \n",
      "gensim-3.8.0         | 18.4 MB   | ######9    |  70% \n",
      "gensim-3.8.0         | 18.4 MB   | #######    |  70% \n",
      "gensim-3.8.0         | 18.4 MB   | #######    |  71% \n",
      "gensim-3.8.0         | 18.4 MB   | #######    |  71% \n",
      "gensim-3.8.0         | 18.4 MB   | #######1   |  72% \n",
      "gensim-3.8.0         | 18.4 MB   | #######2   |  72% \n",
      "gensim-3.8.0         | 18.4 MB   | #######2   |  72% \n",
      "gensim-3.8.0         | 18.4 MB   | #######2   |  73% \n",
      "gensim-3.8.0         | 18.4 MB   | #######3   |  74% \n",
      "gensim-3.8.0         | 18.4 MB   | #######4   |  74% \n",
      "gensim-3.8.0         | 18.4 MB   | #######4   |  75% \n",
      "gensim-3.8.0         | 18.4 MB   | #######5   |  75% \n",
      "gensim-3.8.0         | 18.4 MB   | #######5   |  75% \n",
      "gensim-3.8.0         | 18.4 MB   | #######5   |  76% \n",
      "gensim-3.8.0         | 18.4 MB   | #######5   |  76% \n",
      "gensim-3.8.0         | 18.4 MB   | #######6   |  76% \n",
      "gensim-3.8.0         | 18.4 MB   | #######6   |  76% \n",
      "gensim-3.8.0         | 18.4 MB   | #######6   |  76% \n",
      "gensim-3.8.0         | 18.4 MB   | #######6   |  77% \n",
      "gensim-3.8.0         | 18.4 MB   | #######6   |  77% \n",
      "gensim-3.8.0         | 18.4 MB   | #######6   |  77% \n",
      "gensim-3.8.0         | 18.4 MB   | #######7   |  77% \n",
      "gensim-3.8.0         | 18.4 MB   | #######7   |  77% \n",
      "gensim-3.8.0         | 18.4 MB   | #######7   |  77% \n",
      "gensim-3.8.0         | 18.4 MB   | #######7   |  78% \n",
      "gensim-3.8.0         | 18.4 MB   | #######7   |  78% \n",
      "gensim-3.8.0         | 18.4 MB   | #######7   |  78% \n",
      "gensim-3.8.0         | 18.4 MB   | #######8   |  79% \n",
      "gensim-3.8.0         | 18.4 MB   | #######9   |  79% \n",
      "gensim-3.8.0         | 18.4 MB   | #######9   |  79% \n",
      "gensim-3.8.0         | 18.4 MB   | #######9   |  80% \n",
      "gensim-3.8.0         | 18.4 MB   | ########   |  80% \n",
      "gensim-3.8.0         | 18.4 MB   | ########   |  81% \n",
      "gensim-3.8.0         | 18.4 MB   | ########1  |  81% \n",
      "gensim-3.8.0         | 18.4 MB   | ########2  |  82% \n",
      "gensim-3.8.0         | 18.4 MB   | ########2  |  83% \n",
      "gensim-3.8.0         | 18.4 MB   | ########3  |  83% \n",
      "gensim-3.8.0         | 18.4 MB   | ########3  |  84% \n",
      "gensim-3.8.0         | 18.4 MB   | ########4  |  84% \n",
      "gensim-3.8.0         | 18.4 MB   | ########5  |  85% \n",
      "gensim-3.8.0         | 18.4 MB   | ########5  |  86% \n",
      "gensim-3.8.0         | 18.4 MB   | ########6  |  86% \n",
      "gensim-3.8.0         | 18.4 MB   | ########6  |  87% \n",
      "gensim-3.8.0         | 18.4 MB   | ########6  |  87% \n",
      "gensim-3.8.0         | 18.4 MB   | ########7  |  87% \n",
      "gensim-3.8.0         | 18.4 MB   | ########7  |  87% \n",
      "gensim-3.8.0         | 18.4 MB   | ########7  |  88% \n",
      "gensim-3.8.0         | 18.4 MB   | ########8  |  89% \n",
      "gensim-3.8.0         | 18.4 MB   | ########8  |  89% \n",
      "gensim-3.8.0         | 18.4 MB   | ########9  |  89% \n",
      "gensim-3.8.0         | 18.4 MB   | ########9  |  90% \n",
      "gensim-3.8.0         | 18.4 MB   | ########9  |  90% \n",
      "gensim-3.8.0         | 18.4 MB   | #########  |  90% \n",
      "gensim-3.8.0         | 18.4 MB   | #########  |  90% \n",
      "gensim-3.8.0         | 18.4 MB   | #########  |  91% \n",
      "gensim-3.8.0         | 18.4 MB   | #########  |  91% \n",
      "gensim-3.8.0         | 18.4 MB   | #########1 |  91% \n",
      "gensim-3.8.0         | 18.4 MB   | #########1 |  92% \n",
      "gensim-3.8.0         | 18.4 MB   | #########1 |  92% \n",
      "gensim-3.8.0         | 18.4 MB   | #########2 |  92% \n",
      "gensim-3.8.0         | 18.4 MB   | #########2 |  92% \n",
      "gensim-3.8.0         | 18.4 MB   | #########2 |  93% \n",
      "gensim-3.8.0         | 18.4 MB   | #########2 |  93% \n",
      "gensim-3.8.0         | 18.4 MB   | #########3 |  93% \n",
      "gensim-3.8.0         | 18.4 MB   | #########3 |  94% \n",
      "gensim-3.8.0         | 18.4 MB   | #########3 |  94% \n",
      "gensim-3.8.0         | 18.4 MB   | #########4 |  94% \n",
      "gensim-3.8.0         | 18.4 MB   | #########4 |  94% \n",
      "gensim-3.8.0         | 18.4 MB   | #########4 |  95% \n",
      "gensim-3.8.0         | 18.4 MB   | #########4 |  95% \n",
      "gensim-3.8.0         | 18.4 MB   | #########5 |  95% \n",
      "gensim-3.8.0         | 18.4 MB   | #########5 |  95% \n",
      "gensim-3.8.0         | 18.4 MB   | #########5 |  95% \n",
      "gensim-3.8.0         | 18.4 MB   | #########5 |  96% \n",
      "gensim-3.8.0         | 18.4 MB   | #########5 |  96% \n",
      "gensim-3.8.0         | 18.4 MB   | #########5 |  96% \n",
      "gensim-3.8.0         | 18.4 MB   | #########6 |  96% \n",
      "gensim-3.8.0         | 18.4 MB   | #########6 |  96% \n",
      "gensim-3.8.0         | 18.4 MB   | #########6 |  96% \n",
      "gensim-3.8.0         | 18.4 MB   | #########6 |  97% \n",
      "gensim-3.8.0         | 18.4 MB   | #########7 |  97% \n",
      "gensim-3.8.0         | 18.4 MB   | #########7 |  98% \n",
      "gensim-3.8.0         | 18.4 MB   | #########7 |  98% \n",
      "gensim-3.8.0         | 18.4 MB   | #########8 |  98% \n",
      "gensim-3.8.0         | 18.4 MB   | #########8 |  98% \n",
      "gensim-3.8.0         | 18.4 MB   | #########8 |  99% \n",
      "gensim-3.8.0         | 18.4 MB   | #########9 |  99% \n",
      "gensim-3.8.0         | 18.4 MB   | #########9 | 100% \n",
      "gensim-3.8.0         | 18.4 MB   | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\gurde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gurde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gurde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('datasets\\\\IMDB Dataset.csv')\n",
    "news = pd.read_csv('datasets\\\\RedditNews.csv')\n",
    "twitter = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          4\n",
       "1  A wonderful little production. <br /><br />The...          4\n",
       "2  I thought this was a wonderful way to spend ti...          4\n",
       "3  Basically there's a family where a little boy ...          0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if(data.sentiment[0] not in [1, -1]):\n",
    "    data.sentiment = np.where(data.sentiment == \"positive\", 4, 0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb4a974637724391bbd9b37dd1869ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy Score: 69.22399999999999\n"
     ]
    }
   ],
   "source": [
    "s = SIA()\n",
    "total_values, incorrect = 0, 0\n",
    "for review, sentiment in tqdm(zip(data.review, data.sentiment)):\n",
    "    pred = s.polarity_scores(review)\n",
    "    if(pred['neg'] > pred['pos']):\n",
    "        pred = -1\n",
    "    else:\n",
    "        pred = 1\n",
    "    if(pred != sentiment):\n",
    "        incorrect+=1\n",
    "    total_values+=1\n",
    "print(\"Accuracy Score:\", (total_values-incorrect)/total_values * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 40000 10000 10000\n"
     ]
    }
   ],
   "source": [
    "train_x, test_x, train_y, test_y = tuple(map(lambda x: x.values, train_test_split(data.review, data.sentiment, test_size=0.2, random_state=42)))\n",
    "print(len(train_x), len(train_y), len(test_x), len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That\\'s what I kept asking myself during the many fights, screaming matches, swearing and general mayhem that permeate the 84 minutes. The comparisons also stand up when you think of the one-dimensional characters, who have so little depth that it is virtually impossible to care what happens to them. They are just badly written cyphers for the director to hang his multicultural beliefs on, a topic that has been done much better in other dramas both on TV and the cinema.<br /><br />I must confess, I\\'m not really one for spotting bad performances during a film, but it must be said that Nichola Burley (as the heroine\\'s slutty best friend) and Wasim Zakir (as the nasty, bullying brother) were absolutely terrible. I don\\'t know what acting school they graduated from, but if I was them I\\'d apply for a full refund post haste. Only Samina Awan in the lead role manages to impress in a cast of so-called British talent that we\\'ll probably never hear from again. At least, that\\'s the hope. Next time, hire a different scout.<br /><br />Another intriguing thought is the hideously fashionable soundtrack featuring the likes of Snow Patrol, Ian Brown and Keane. Now, I\\'m a bit of a music fan and I\\'m familiar with most of these artists output, but I didn\\'t recognise any of the tracks during this movie (apart from the omnipresent \"Run\"). B-sides, anyone? We get many, many musical montages which telegraph how we\\'re suppose to feel. These are accompanied by such startlingly original images as couples kissing by a swollen lake and canoodling in doorways. This is a problem, as none of the songs convey the mood efficiently, and we realise the director lacks the ability to carry the emotional journey to the audience through storytelling and dialogue alone.<br /><br />The ending is presumably meant to be just desserts, as everybody gets their comeuppance and there is at least one big shock in store.. But I remained resolutely unmoved because the script had given me no-one to root for. It\\'s not enough to tackle a hot-button issue, you have to actually give us a plot that hasn\\'t already been done to death and individuals who are more than window dressing. As it stands, this film is a noble failure, with only the promising lead actress and a few mildly diverting punch-ups to save it from the bin. 4/10. Must try harder..'\n",
      " \"I did not watch the entire movie. I could not watch the entire movie. I stopped the DVD after watching for half an hour and I suggest anyone thinking of watching themselves it stop themselves before taking the disc out of the case.<br /><br />I like Mafia movies both tragic and comic but Corky Romano can only be described as a tragic attempt at a mafia comedy.<br /><br />The problem is Corky Romano simply tries too hard to get the audience to laugh, the plot seems to be an excuse for moving Chris Kattan (Corky) from one scene to another. Corky himself is completely overplayed and lacks subtlety or credulity - all his strange mannerisms come across as contrived - Chris Kattan is clearly 'acting' rather than taking a role - it bounces you right out of the story. Each scene is utterly predictable, the 'comedic event' that will occur on the set is obvious as soon as each scene is introduced. In comedies such as Mr. Bean the disasters caused by the title character are funny because you can empathise with the characters motivations and initial event and the situation the character ends up in is not telegraphed. Corky however gives the feeling that he is deliberately screwing up in a desperate attempt to draw a laugh from the audience.<br /><br />If Chris had not played such an alien character (who never really connects with the other characters in the movie) and whose behaviour is entirely inexplicable (except for trying to draw laughs) and the comedy scenes weren't so predictable and stereotyped - all the jokes seemed far too familiar) this movie could have been watchable. But it isn't. Don't watch it.\"\n",
      " \"A touching love story reminiscent of \\x91In the Mood for Love'. Drawing heavily on Chinese poetry and how this is used by eastern people to communicate feelings to each other, the story focuses on a schoolteacher who wants so much to be a model teacher as well as a good husband and father. A senior student is very attracted to him. As the story unfolds we see the emotions below the surface in his 20 year marriage and how he grapples with the moral dilemmas that face him. A beautiful and moving story.\"\n",
      " ...\n",
      " \"Avoid this one! It is a terrible movie. So what if it is very exciting? All it is is just pointless murders. And the whole thing with Thorn and Michael's curse, that was the absolute worst thing they could possibly do to the series! Why couldn't they leave Michael's story a mystery? He's supposed to be the Boogeyman, not part of some stupid cult!! Ugh! Thank God for Halloween H20, which wiped out Halloween 3-6! They all sucked! But anyway, if you see this movie, please expect no more than pointless murders and gore.\"\n",
      " \"This production was quite a surprise for me. I absolutely love obscure early 30s movies, but I wasn't prepared for the last 25 minutes of this story. If, by any chance, you're not convinced in the first half, hang in there for the finale. Of course, you must look at the blatant racism as being purely topical. A fascinating viewing experience, but I think THE CAT'S PAW is not available on video/DVD yet. Watch your PBS listings!\"\n",
      " 'This is a decent movie. Although little bit short in time for me, it packs a lot of action, grit, commonsense and emotions in that time frame. Matt Dillon and the other main character does a great job in this movie. The emotions and intensity were convincing and tense throughout the movie. It is not typical fancy expensive Hollywood CGI action movie, but it was a very satisfying movie indeed for the price. My evening was great because of this movie. This movie is straight traditional action movie with great acting, story and directing. I would recommend this movie. The character development of the characters were good and makes you believe that were are actually seeing a real event taking place. Because this movie I believe was made with cheaper budget, the acting and quality were much higher.']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.902"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "('vectorizer', CountVectorizer(analyzer=\"word\",\n",
    "ngram_range=(1, 2),\n",
    "tokenizer=word_tokenize,\n",
    "preprocessor=clean_text)),\n",
    "('classifier', LinearSVC(max_iter=2500, dual=False, random_state=42, verbose=2))])\n",
    "clf.fit(train_x, train_y)\n",
    "clf.score(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_dict = {0:\"negative\", 2:\"neutral\", 4:\"positive\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start\n",
    "imdb_data = pd.read_csv('datasets\\\\IMDB Dataset.csv')\n",
    "twitter_data = pd.read_csv('datasets\\\\training.1600000.processed.noemoticon.csv', \n",
    "                           names=[\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"], \n",
    "                           encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared IMDB Dataset\n",
      "                                               review  sentiment\n",
      "0  One of the other reviewers has mentioned that ...          4\n",
      "1  A wonderful little production. <br /><br />The...          4\n",
      "2  I thought this was a wonderful way to spend ti...          4\n",
      "3  Basically there's a family where a little boy ...          0\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...          4\n",
      "Prepared twitter Dataset\n",
      "    target                                               text\n",
      "0       0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
      "1       0  is upset that he can't update his Facebook by ...\n",
      "2       0  @Kenichan I dived many times for the ball. Man...\n",
      "3       0    my whole body feels itchy and like its on fire \n",
      "4       0  @nationwideclass no, it's not behaving at all....\n"
     ]
    }
   ],
   "source": [
    "# preparing data for training\n",
    "if(imdb_data.sentiment[0] not in [4, 0]):\n",
    "    imdb_data['sentiment'].replace({\"positive\":4, \"negative\":0}, inplace=True)\n",
    "    #imdb_data.sentiment = np.where(imdb_data.sentiment == \"positive\", 1, -1)\n",
    "print(\"Prepared IMDB Dataset\\n\", imdb_data.head())\n",
    "\n",
    "if(\"flag\" in twitter_data.keys()):\n",
    "    twitter_data = twitter_data.drop(['user', 'flag', 'date', 'ids'], axis=1)\n",
    "print(\"Prepared twitter Dataset\\n\", twitter_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for cleaning text\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"}\n",
    "\n",
    "def clean_text(text, remove_stopwords = False):\n",
    "    text = text.lower()\n",
    "    \n",
    "    if(True):\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "\n",
    "    text = re.sub(r\"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\", \" \", text)\n",
    "    text = re.sub(r\"<br />\", \" \", text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'0,0', '00', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|.,+&=*%.,!?:#@\\[\\]]', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    text = re.sub(r'\\$', ' $ ', text)\n",
    "    text = re.sub(r'u s ', ' united states ', text)\n",
    "    text = re.sub(r'u n ', ' united nations ', text)\n",
    "    text = re.sub(r'u k ', ' united kingdom ', text)\n",
    "    text = re.sub(r'j k ', ' jk ', text)\n",
    "    text = re.sub(r' s ', ' ', text)\n",
    "    text = re.sub(r' yr ', ' year ', text)\n",
    "    text = re.sub(r' l g b t ', ' lgbt ', text)\n",
    "    text = re.sub(r'0km ', '0 km ', text)\n",
    "\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sia(string, ret_decision=False):\n",
    "    if(ret_decision == True):\n",
    "        score = SIA().polarity_scores(string)['compound']\n",
    "        if(score < -0.2):\n",
    "            return 0\n",
    "        elif(score > 0.2):\n",
    "            return 4\n",
    "        return 2\n",
    "    return SIA().polarity_scores(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
      "Cleaned Text:     awww that is a bummer you shoulda got david carr of third day to do it d\n",
      "SIA: {'neg': 0.157, 'neu': 0.843, 'pos': 0.0, 'compound': -0.3818}\n",
      "Original Sentiment: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Text:\", twitter_data.text[0])\n",
    "print(\"Cleaned Text:\", clean_text(twitter_data.text[0]))\n",
    "print(\"SIA:\", sia(clean_text(twitter_data.text[0])))\n",
    "print(\"Original Sentiment:\", twitter_data.target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   target  1600000 non-null  int64 \n",
      " 1   text    1600000 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 24.4+ MB\n"
     ]
    }
   ],
   "source": [
    "twitter_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543d711a43464a14a832309c4ec967d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 37.18\n"
     ]
    }
   ],
   "source": [
    "total, correct = 0, 0\n",
    "data = twitter_data[:5000]\n",
    "for i in tqdm(range(int(len(data)))):\n",
    "    if(sia(clean_text(twitter_data['text'][i]), True) == twitter_data['target'][i]):\n",
    "        correct+=1\n",
    "    total+=1\n",
    "print(\"Accuracy:\", correct/total*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "0      One of the other reviewers has mentioned that ...          4\n",
       "1      A wonderful little production. <br /><br />The...          4\n",
       "2      I thought this was a wonderful way to spend ti...          4\n",
       "3      Basically there's a family where a little boy ...          0\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...          4\n",
       "...                                                  ...        ...\n",
       "49995  I thought this movie did a down right good job...          4\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...          0\n",
       "49997  I am a Catholic taught in parochial elementary...          0\n",
       "49998  I'm going to have to disagree with the previou...          0\n",
       "49999  No one expects the Star Trek movies to be high...          0\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1650000,), (1650000,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = np.concatenate((np.asarray(twitter_data['text']), imdb_data['review']), axis=0)\n",
    "sentiment_data = np.concatenate((np.asarray(twitter_data['target']), imdb_data['sentiment']), axis=0)\n",
    "text_data.shape, sentiment_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1485000 1485000 165000 165000\n"
     ]
    }
   ],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(text_data, \n",
    "                                                    sentiment_data, \n",
    "                                                    test_size=0.10, \n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=42)\n",
    "#train_x = train_x[:100000]\n",
    "#test_x = test_x[:10000]\n",
    "#train_y = train_y[:100000]\n",
    "#test_y = test_y[:10000]\n",
    "print(len(train_x), len(train_y), len(test_x), len(test_y))\n",
    "#best iter = 2500 for 50000, 100000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gurde\\Anaconda3\\envs\\deloitte\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 16s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error={'strict'},\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 2),\n",
       "                preprocessor=<function clean_text at 0x00000262C8B718B8>,\n",
       "                stop_words=None, strip_accents=None,\n",
       "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=<function word_tokenize at 0x00000262C61A8DC8>,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf = Pipeline([\n",
    "                ('vectorizer', CountVectorizer(analyzer=\"word\",\n",
    "                                               ngram_range=(1, 2),\n",
    "                                               tokenizer=word_tokenize,\n",
    "                                               #tokenizer=lambda text: mark_negation(word_tokenize(text)),\n",
    "                                               preprocessor=clean_text,\n",
    "                                               decode_error={'strict'})),#encoding=\"ISO-8859-1\"\n",
    "                ('classifier', LinearSVC(max_iter=1000, dual=False, random_state=42))])\n",
    "clf['vectorizer'].fit(train_x, train_y)\n",
    "#clf.score(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['0'], dtype='<U145')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf['vectorizer'].inverse_transform([\"Iran Bombed US\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = clf['vectorizer']\n",
    "n_clf = Pipeline([('vectorizer', vectorizer),\n",
    "                  ('classifier', LinearSVC(max_iter=1000, dual=False, random_state=42))])\n",
    "n_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689d332c5de742e99f96878c37745fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with iterations:  2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gurde\\Anaconda3\\envs\\deloitte\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with iterations:  3000\n",
      "Training model with iterations:  3500\n",
      "\n",
      "[77.98, 77.94, 77.91]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of passed values is 3, index implies 8.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-192-f9e2a252ad3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Convert the accuracy list to a series\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m250\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;31m# Set the plot size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deloitte\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    290\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m                         raise ValueError(\n\u001b[1;32m--> 292\u001b[1;33m                             \u001b[1;34mf\"Length of passed values is {len(data)}, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m                             \u001b[1;34mf\"index implies {len(index)}.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m                         )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of passed values is 3, index implies 8."
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "\n",
    "# Iterate along a logarithmically spaced ranged\n",
    "for i in tqdm(range(2500, 4000, 500)):\n",
    "    # Print out the number of iterations to use for the current loop\n",
    "    print('Training model with iterations: ', i)\n",
    "    clf = Pipeline([\n",
    "                ('vectorizer', CountVectorizer(analyzer=\"word\",\n",
    "                                               ngram_range=(1, 2),\n",
    "                                               tokenizer=word_tokenize,\n",
    "                                               #tokenizer=lambda text: mark_negation(word_tokenize(text)),\n",
    "                                               preprocessor=clean_text,\n",
    "                                               decode_error={'strict'})),#encoding=\"ISO-8859-1\"\n",
    "                ('classifier', LinearSVC(max_iter=i, dual=False, random_state=42))])\n",
    "    # Fit the algorithm to the data\n",
    "    clf.fit(train_x, train_y)\n",
    "    # Append the current accuracy score to the template list\n",
    "    acc.append(accuracy_score(test_y, clf.predict(test_x)) * 100)\n",
    "print(acc)\n",
    "# Convert the accuracy list to a series\n",
    "acc = pd.Series(acc, index = list(range(1000, 3000, 250)))\n",
    "# Set the plot size\n",
    "plt.figure(figsize = (15,10))\n",
    "# Set the plot title\n",
    "title = 'Graph to show the accuracy of the SVC model as number of iterations increases\\nfinal accuracy: ' + str(acc.iloc[-1])\n",
    "plt.title(title)\n",
    "# Set the xlabel and ylabel\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Accuracy score')\n",
    "# Plot the graph\n",
    "acc.plot.line()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input',\n",
       " 'encoding',\n",
       " 'decode_error',\n",
       " 'strip_accents',\n",
       " 'preprocessor',\n",
       " 'tokenizer',\n",
       " 'analyzer',\n",
       " 'lowercase',\n",
       " 'token_pattern',\n",
       " 'stop_words',\n",
       " 'max_df',\n",
       " 'min_df',\n",
       " 'max_features',\n",
       " 'ngram_range',\n",
       " 'vocabulary',\n",
       " 'binary',\n",
       " 'dtype',\n",
       " 'fixed_vocabulary_',\n",
       " '_stop_words_id',\n",
       " 'stop_words_',\n",
       " 'vocabulary_',\n",
       " '__module__',\n",
       " '__doc__',\n",
       " '__init__',\n",
       " '_sort_features',\n",
       " '_limit_features',\n",
       " '_count_vocab',\n",
       " 'fit',\n",
       " 'fit_transform',\n",
       " 'transform',\n",
       " 'inverse_transform',\n",
       " 'get_feature_names',\n",
       " '_more_tags',\n",
       " '_white_spaces',\n",
       " 'decode',\n",
       " '_word_ngrams',\n",
       " '_char_ngrams',\n",
       " '_char_wb_ngrams',\n",
       " 'build_preprocessor',\n",
       " 'build_tokenizer',\n",
       " 'get_stop_words',\n",
       " '_check_stop_words_consistency',\n",
       " '_validate_custom_analyzer',\n",
       " 'build_analyzer',\n",
       " '_validate_vocabulary',\n",
       " '_check_vocabulary',\n",
       " '_validate_params',\n",
       " '_warn_for_unused_params',\n",
       " '__dict__',\n",
       " '__weakref__',\n",
       " '__repr__',\n",
       " '__hash__',\n",
       " '__str__',\n",
       " '__getattribute__',\n",
       " '__setattr__',\n",
       " '__delattr__',\n",
       " '__lt__',\n",
       " '__le__',\n",
       " '__eq__',\n",
       " '__ne__',\n",
       " '__gt__',\n",
       " '__ge__',\n",
       " '__new__',\n",
       " '__reduce_ex__',\n",
       " '__reduce__',\n",
       " '__subclasshook__',\n",
       " '__init_subclass__',\n",
       " '__format__',\n",
       " '__sizeof__',\n",
       " '__dir__',\n",
       " '__class__',\n",
       " '_get_param_names',\n",
       " 'get_params',\n",
       " 'set_params',\n",
       " '__getstate__',\n",
       " '__setstate__',\n",
       " '_get_tags']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf['vectorizer'].__dir__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77774"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(np.asarray(imdb_data.review[:]), np.asarray(imdb_data.sentiment[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve, learning_curve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95a74bb919a4a85a9be5497791e8129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241c7d1388d545a296721a5f91d6f3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-195-fe0904ddb4ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtest_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtrain_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mtest_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deloitte\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deloitte\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m             \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deloitte\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1270\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1271\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1272\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deloitte\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1168\u001b[0m                           \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindptr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1169\u001b[0m                           dtype=self.dtype)\n\u001b[1;32m-> 1170\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1171\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deloitte\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36msort_indices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_sorted_indices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m             _sparsetools.csr_sort_indices(len(self.indptr) - 1, self.indptr,\n\u001b[1;32m-> 1137\u001b[1;33m                                           self.indices, self.data)\n\u001b[0m\u001b[0;32m   1138\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_sorted_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_scores = np.empty(test_y.shape, dtype='int32')\n",
    "train_scores = np.empty(train_y.shape, dtype='int32')\n",
    "for i, (text, target) in tqdm(enumerate(zip(test_x, test_y))):\n",
    "    test_scores[i] = clf.predict([text])\n",
    "for i, (text, target) in tqdm(enumerate(zip(train_x, train_y))):\n",
    "    train_scores[i] = clf.predict([text])\n",
    "test_scores, train_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.asarray(pd.read_csv('datasets\\\\RedditNews.csv').News)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64bdebe7d2504c9c855beb2ed3c62582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News Headline: After deploying 3,000 troops and spending hundreds of millions of dollars, only 28 Ebola patients have been treated at the 11 centers built by the American military in Liberia\n",
      "SIA: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "4\n",
      "News Headline: 46 Ukrainian banks declare insolvency:Dozens of Ukrainian banks have reportedly gone bankrupt over the course of one year.\n",
      "SIA: {'neg': 0.175, 'neu': 0.825, 'pos': 0.0, 'compound': -0.5574}\n",
      "0\n",
      "News Headline: More than 100 hand grenades and around a dozen automatic weapons on their way to Stockholm were confiscated on Friday when Bosnian police swooped down in on a weapons ring that had been on law enforcements radar since last year.\n",
      "SIA: {'neg': 0.131, 'neu': 0.791, 'pos': 0.078, 'compound': -0.3262}\n",
      "0\n",
      "News Headline: German foreign minister rejects calls to invite Putin to G7 talks\n",
      "SIA: {'neg': 0.232, 'neu': 0.652, 'pos': 0.116, 'compound': -0.3818}\n",
      "4\n",
      "News Headline: South African Author ZP Dala Reportedly Taken to Mental Institution After Refusing to Renounce Salman Rushdie Comments\n",
      "SIA: {'neg': 0.144, 'neu': 0.856, 'pos': 0.0, 'compound': -0.4019}\n",
      "4\n",
      "News Headline: Raul Castro: The United States owes a historic debt to Cuba\n",
      "SIA: {'neg': 0.188, 'neu': 0.602, 'pos': 0.211, 'compound': 0.0772}\n",
      "4\n",
      "News Headline: Saudi border guards killed in mortar attack\n",
      "SIA: {'neg': 0.603, 'neu': 0.397, 'pos': 0.0, 'compound': -0.8225}\n",
      "4\n",
      "News Headline: Canada's biggest uranium producer Cameco to supply Nuclear Fuel to India\n",
      "SIA: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "4\n",
      "News Headline: ISIS fighters blindfold gay man and stone him to death.\n",
      "SIA: {'neg': 0.389, 'neu': 0.611, 'pos': 0.0, 'compound': -0.6249}\n",
      "4\n",
      "News Headline: Coast Guard, Ottawa defend Vancouver fuel spill response - 7 hrs after toxic fuel leak on English Bay was 1st reported, a boom was set up around the affected vessel  a response the citys mayor &amp; B.C. Premier call inadequate, although the Coast Guard says it was exceptional.\n",
      "SIA: {'neg': 0.14, 'neu': 0.86, 'pos': 0.0, 'compound': -0.6908}\n",
      "4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#total, correct = 0, 0\n",
    "for news in tqdm(data[11140:11150]):\n",
    "    print(\"News Headline:\", news)\n",
    "    print(\"SIA:\", sia(news))\n",
    "    print(l_clf.predict([news])[0])\n",
    "    #if(sia(clean_text(news), True) == clf.predict([news])[0]):\n",
    "    #    if(correct < 10):\n",
    "    #        print(\"News Headline:\", news)\n",
    "    #        print(\"SIA:\", sia(news))\n",
    "    #        print(sentiment_dict[clf.predict([news])[0]])\n",
    "    #    elif(correct >= 10):\n",
    "    #        break\n",
    "    #    correct+=1\n",
    "    #total+=1\n",
    "        \n",
    "#print(\"Accuracy:\", (correct/total)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imdb_clf.joblib']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import joblib\n",
    "joblib.dump(clf, \"test.joblib\", compress=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import joblib\n",
    "l_clf = joblib.load(\"test_model1.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After [4]\n",
      "deploying [4]\n",
      "3,000 [0]\n",
      "troops [4]\n",
      "and [4]\n",
      "spending [4]\n",
      "hundreds [4]\n",
      "of [4]\n",
      "millions [4]\n",
      "of [4]\n",
      "dollars, [4]\n",
      "only [4]\n",
      "28 [4]\n",
      "Ebola [4]\n",
      "patients [0]\n",
      "have [4]\n",
      "been [4]\n",
      "treated [4]\n",
      "at [4]\n",
      "the [4]\n",
      "11 [4]\n",
      "centers [4]\n",
      "built [4]\n",
      "by [4]\n",
      "the [4]\n",
      "American [4]\n",
      "military [4]\n",
      "in [4]\n",
      "Liberia [4]\n"
     ]
    }
   ],
   "source": [
    "line = \"After deploying 3,000 troops and spending hundreds of millions of dollars, only 28 Ebola patients have been treated at the 11 centers built by the American military in Liberia\"\n",
    "for i in line.split(\" \"):\n",
    "    print(i, l_clf.predict([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
